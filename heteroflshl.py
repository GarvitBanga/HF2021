# -*- coding: utf-8 -*-
"""HeteroFlbasicCiFar10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zmFxnshPhkgKHKPnycF-TMMmG2adGoMu
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from os import listdir
from tensorflow.keras.preprocessing import sequence
import tensorflow as tfs
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten
from tensorflow.keras.layers import LSTM

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint

from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D,LeakyReLU,MaxPool2D
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os
import cv2, random

height=32
width=32
depth=3

inputShape = (height, width, depth)

# Prepare the train and test dataset.
from tensorflow import keras
# batch_size = 64
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
print(x_train.shape)
# Normalize data
x_train = x_train.astype("float32") / 255.0
x_train = np.reshape(x_train, (-1, 32, 32, 3))

x_test = x_test.astype("float32") / 255.0
x_test = np.reshape(x_test, (-1, 32, 32, 3))
y_train=np.reshape(y_train,(-1))
y_test=np.reshape(y_test,(-1))

from  keras.utils import np_utils
# y_train = np_utils.to_categorical(y_train)
# y_test= np_utils.to_categorical(y_test)
print(y_train[0])
print(y_train.shape,y_test.shape)
print(x_train.shape)
print(y_train[0],x_train[0])

from sklearn.utils import shuffle
x_train, y_train= shuffle(x_train, y_train, random_state=42)
X=[]
Y=[]
j=0
for i in range(20):
  X.append(x_train[j:j+2500])
  Y.append(y_train[j:j+2500])
  j+=2500

X=np.array(X)
Y=np.array(Y)
print(X.shape,Y.shape)

from tensorflow.keras.layers import BatchNormalization
class MODEL:
    @staticmethod
    def build(nl):
        # model = Sequential()
        # model.add(Conv2D(32/nl,(3,3),strides=(2, 2), padding="same",input_shape=[32,32,3]) )
        # model.add(Conv2D(32/nl,(3,3),strides=(2, 2), padding="same") )
        # model.add(MaxPooling2D(pool_size=(2, 2),strides=2))
        # model.add(Conv2D(64/nl,(3,3),strides=(2, 2), padding="same") )
        # model.add(Conv2D(64/nl,(3,3),strides=(2, 2), padding="same") )
        # model.add(MaxPooling2D(pool_size=(2, 2),strides=2))

        # # model.add(Conv2D(512/nl,(3,3),strides=(2, 2), padding="same") )
        # # model.add(LeakyReLU(alpha=0.2))
        # # model.add(MaxPooling2D(pool_size=(2, 2)))
        # # model.add(Dropout(0.25))

        # # model.add(Conv2D(512/nl, (3, 3), strides=(2, 2), padding="same") )
        # model.add(Dropout(0.5))
        # model.add(Flatten())
        # model.add(Dense(128/nl,activation='relu'))
        # model.add(Dense(10,activation='softmax'))#for output layer

        # model = Sequential()
        # model.add(Conv2D(64/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
        # model.add(Conv2D(64/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        # model.add(MaxPooling2D((2, 2)))
        # model.add(Conv2D(128/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        # model.add(Conv2D(128/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        # model.add(MaxPooling2D((2, 2)))
        # model.add(Flatten())
        # model.add(Dense(256/nl, activation='relu', kernel_initializer='he_uniform'))
        # model.add(Dense(10))
        model = Sequential()
        model.add(Conv2D(128/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
        model.add(BatchNormalization())
        model.add(Conv2D(128/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        model.add(BatchNormalization())
        model.add(MaxPool2D((2, 2)))
        model.add(Dropout(0.2))
        model.add(Conv2D(256/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        model.add(BatchNormalization())
        model.add(Conv2D(256/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        model.add(BatchNormalization())
        model.add(MaxPool2D((2, 2)))
        model.add(Dropout(0.3))
        model.add(Conv2D(512/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        model.add(BatchNormalization())
        model.add(Conv2D(512/nl, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
        model.add(BatchNormalization())
        model.add(MaxPool2D((2, 2)))
        model.add(Dropout(0.4))
        model.add(Flatten())
        model.add(Dense(512/nl, activation='relu', kernel_initializer='he_uniform'))
        model.add(BatchNormalization())
        model.add(Dropout(0.5))
        model.add(Dense(10))

        return model

from sklearn.metrics import accuracy_score
def test_model(X_test, Y_test,  model, comm_round):
    # cce = tf.keras.losses.SparseCategoricalCrossentropy()
    # #logits = model.predict(X_test, batch_size=100)
    # logits = model.predict(X_test)
    # loss = cce(Y_test, logits)
    # acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    # print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    model.compile(  optimizer=keras.optimizers.Adam(),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[keras.metrics.SparseCategoricalAccuracy()],
)
    # loss,acc=model.evaluate(x_test,y_test)
    acc,loss=model.evaluate(x_test,y_test)
    return acc, loss
def scale_model_weights(weight, scalar):
    '''function for scaling a models weights'''
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final

def sum_scaled_weights(scaled_weight_list):
    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''
    avg_grad = list()
    #get the average grad accross all client gradients
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
      
    return avg_grad

from tensorflow import keras
from tensorflow.keras.models import clone_model

global1= MODEL()
global_model = global1.build(1)
comms_round = 50
# print(trainy[2].shape)
from tensorflow.keras import backend as K
global11= MODEL()
global_model1 = global11.build(1)
f= open("MyFileHeteroCifar10.txt", "a+") 
f.close()
epoch=[np.random.randint(26,31),np.random.randint(20,26),np.random.randint(10,21)]
print(epoch)
for comm_round in range(150):
  f= open("MyFileHeteroCifar10.txt", "a+") 
  print("Comm_Round:",comm_round)

  global_weights = global_model.get_weights()
  global_model1.set_weights(global_weights)
  scaled_local_weight_list = list()

  clientcluster=[[0,1,2,3,4],[5,6,7,8,9,10],[11,12,13,14,15,16,17,18,19]]
  downsize=8
  inpchannel=-1
  opchannel=-1
  index=0
  clustr=3
  for clstr in reversed(clientcluster):
    clustr=clustr-1
    downsize=downsize/2
    cluster=MODEL()
    cluster_model=cluster.build(downsize)
    print("CLuster NO.",index)
    f.write("CLuster NO. %d \n" %index)
    index=index+1

    nmlayers=len(cluster_model.layers)
    for i in range(0,nmlayers):
      if(len(cluster_model.layers[i].get_weights())==2):
        print(i)
        wt,bs=cluster_model.layers[i].get_weights()
        wt1,bs1=global_model.layers[i].get_weights()
        
        # print(wt.shape[2],bs.shape[0])
        # print(wt2.shape,bs2.shape)
        # print(wt.shape)
        if(i==0):
          wt=wt1[:wt.shape[0],:wt.shape[1],:wt.shape[2],:wt.shape[3]]
          # bs=bs1[:bs.shape[0]]
        elif(i==nmlayers-4):
          wt=wt1[:wt.shape[0],:wt.shape[1]]
          # bs=bs1[:bs.shape[0]]

        elif(i==nmlayers-1):
          wt=wt1[:wt.shape[0],:wt.shape[1]]
          # bs=bs1[:bs.shape[0]]
        else:
          wt=wt1[:wt.shape[0],:wt.shape[1],:wt.shape[2],:wt.shape[3]]
          # bs=bs1[:bs.shape[0]]
        
        cluster_model.layers[i].set_weights([wt,bs])
      # if(len(cluster_model.layers[i].get_weights())==4):
      #   a,b,c,d=cluster_model.layers[i].get_weights()
      #   a1,b1,c1,d1=global_model.layers[i].get_weights()
      #   print(a.shape[0])
      #   a=a1[:a.shape[0]]
      #   b=b1[:b.shape[0]]
      #   c=c1[:c.shape[0]]
      #   d=d1[:d.shape[0]]
      #   cluster_model.layers[i].set_weights([a,b,c,d])











    for globalrounds in range(1):
      print("Cluster Rounds:",globalrounds)
      cluster_weights=cluster_model.get_weights()
      scaled_local_weight_list = list()
      for clients in clstr:
        print("Client No:",clients)
        local = MODEL()
        local_model=local.build(downsize)
        local_model.compile(optimizer=keras.optimizers.Adam(),
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )
        
        local_model.set_weights(cluster_weights)
        #trainy[ind]=np.array(trainy[ind]).reshape(-1,1)
        history=local_model.fit(X[clients],Y[clients], epochs=epoch[clustr])#,validation_data=(x_test,y_test))
        # print("Accuracy: ",history.history["accuracy"][4])
        # local_model.evaluate(x_test, y_test)
        local_loss, local_acc = test_model(x_test,y_test, local_model, comm_round)
        f.write("Communication Round: %d Client No: %d Local Model ACCURACY: %f LOSS: %f \n" %(comm_round ,clients,local_acc ,local_loss))
        scaling_factor=1.0/len(clstr) #1/no.ofclients
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        K.clear_session()
      average_weights = sum_scaled_weights(scaled_local_weight_list)
      #global_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])

      nmlayers=len(cluster_model.layers)
      tmp_model=MODEL.build(downsize)
      tmp_model.set_weights(cluster_model.get_weights())
      # print(cluster_model.layers[4].get_weights())


      cluster_model.set_weights(average_weights)
      # print(cluster_model.layers[4].get_weights())
      if(clstr==clientcluster[-1]):
        for i in range(1,nmlayers-1):
          if(len(cluster_model.layers[i].get_weights())==2):
            wt,bs=global_model1.layers[i].get_weights()
            wt1,bs1=cluster_model.layers[i].get_weights()
            # wt2,bs2=prev_model.layers[i].get_weights()
            print(i)
            # print(wt.shape[2],bs.shape[0])
            # print(wt2.shape,bs2.shape)
            print(wt.shape)
            # wt[:,:,:wt2.shape[2],:wt2.shape[3]]=wt2[:,:,:wt2.shape[2],:wt2.shape[3]]
            
            if(i==nmlayers-4):
              wt[:wt1.shape[0],:wt1.shape[1]]=wt1
              # bs[:bs1.shape[0]]=bs1

            else:
              wt[:wt1.shape[0],:wt1.shape[1],:wt1.shape[2],:wt1.shape[3]]=wt1
              # bs[:bs1.shape[0]]=bs1
            global_model1.layers[i].set_weights([wt,bs])
          # if(len(cluster_model.layers[i].get_weights())==4):
          #   a,b,c,d=cluster_model.layers[i].get_weights()
          #   a1,b1,c1,d1=global_model.layers[i].get_weights()
          #   a1[:a.shape[0]]=a
          #   b1[:b.shape[0]]=b
          #   c1[:c.shape[0]]=c
          #   d1[:d.shape[0]]=d
          #   global_model.layers[i].set_weights([a1,b1,c1,d1])



      elif(clstr!=clientcluster[-1]):
        for i in range(1,nmlayers-1):
          if(len(cluster_model.layers[i].get_weights())==2):
            wt,bs=global_model1.layers[i].get_weights()
            wt1,bs1=cluster_model.layers[i].get_weights()
            wt2,bs2=prev_model.layers[i].get_weights()
            print(i)
            # print(wt.shape[2],bs.shape[0])
            print(wt2.shape,bs2.shape)
            print(wt1.shape)
            # wt[:,:,:wt2.shape[2],:wt2.shape[3]]=wt2[:,:,:wt2.shape[2],:wt2.shape[3]]
            
            # if(i==0):
            #   wt[:,:,:,wt2.shape[3]:wt1.shape[3]]=wt1[:,:,:,wt2.shape[3]:wt1.shape[3]]
            #   # bs[bs2.shape[0]:bs1.shape[0]]=bs1[bs2.shape[0]:bs1.shape[0]]
            # el
            if(i==nmlayers-4):
              wt[wt2.shape[0]:wt1.shape[0],wt2.shape[1]:wt1.shape[1]]=wt1[wt2.shape[0]:wt1.shape[0],wt2.shape[1]:wt1.shape[1]]
              # bs[bs2.shape[0]:bs1.shape[0]]=bs1[bs2.shape[0]:bs1.shape[0]]

            # elif(i==nmlayers-1):
            #   wt[wt2.shape[0]:wt1.shape[0],:]=wt1[wt2.shape[0]:wt1.shape[0],:]
            #   # bs[bs2.shape[0]:bs1.shape[0]]=bs1[bs2.shape[0]:bs1.shape[0]]
            else:
              wt[wt2.shape[0]:wt1.shape[0],wt2.shape[1]:wt1.shape[1],wt2.shape[2]:wt1.shape[2],wt2.shape[3]:wt1.shape[3]]=wt1[wt2.shape[0]:wt1.shape[0],wt2.shape[1]:wt1.shape[1],wt2.shape[2]:wt1.shape[2],wt2.shape[3]:wt1.shape[3]]
              # bs[bs2.shape[0]:bs1.shape[0]]=bs1[bs2.shape[0]:bs1.shape[0]]
            global_model1.layers[i].set_weights([wt,bs])
          # if(len(cluster_model.layers[i].get_weights())==4):
          #   a,b,c,d=cluster_model.layers[i].get_weights()
          #   a1,b1,c1,d1=global_model.layers[i].get_weights()
          #   a2,b2,c2,d2=prev_model.layers[i].get_weights()
          #   a1[a2.shape[0]:a.shape[0]]=a[a2.shape[0]:a.shape[0]]
          #   b1[b2.shape[0]:b.shape[0]]=b[b2.shape[0]:b.shape[0]]
          #   c1[c2.shape[0]:c.shape[0]]=c[c2.shape[0]:c.shape[0]]
          #   d1[d2.shape[0]:d.shape[0]]=d[d2.shape[0]:d.shape[0]]
          #   global_model.layers[i].set_weights([a1,b1,c1,d1])
      prev_model=MODEL.build(downsize)
      prev_model.set_weights(cluster_model.get_weights())

      cluster_loss, cluster_acc = test_model(x_test,y_test, cluster_model, comm_round)
      f.write("\nCLuster NO. %d Communication Round: %d Cluster Model ACCURACY: %f LOSS: %f \n" %(index,comm_round ,cluster_acc ,cluster_loss))


  # global_model.set_weights(c_model.get_weights())
  global_model.set_weights(global_model1.get_weights())
  global_loss, global_acc = test_model(x_test,y_test, global_model, comm_round)
  f.write("\nCommunication Round: %d GLOBAL MODEL ACCURACY: %f LOSS: %f \n" %(comm_round ,global_acc ,global_loss))
  f.close()








      # print(cluster_model.layers[4].get_weights())


  















#     print("CLIENT NO: ",ind)
    
#     if(ind>5):
#       for lcm_round in range(1):

#       local = MODEL()
#       local_model=local.build(32)
#     elif(ind>=2 and ind<=5):
#       local_model=local.build(16)
#     else:
#       local_model=local.build(8)

    
#     local_model.compile(optimizer=keras.optimizers.Adam(),
#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
#     metrics=[keras.metrics.SparseCategoricalAccuracy()],
# )
    
#     local_model.set_weights(global_weights[:4][:4])
#     #trainy[ind]=np.array(trainy[ind]).reshape(-1,1)
#     history=local_model.fit(X[ind],Y[ind], epochs=1)#,validation_data=(x_test,y_test))
#     # print("Accuracy: ",history.history["accuracy"][4])
#     # local_model.evaluate(x_test, y_test)
#     scaling_factor=1.0/5 #1/no.ofclients
#     scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
#     scaled_local_weight_list.append(scaled_weights)
#     K.clear_session()
#   average_weights = sum_scaled_weights(scaled_local_weight_list)
#   #global_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
#   global_model.set_weights(average_weights)
#   #val_acc,val_loss=global_model.evaluate(x_train,y_train)
#   #print(val_loss,val_acc)
#   global_acc, global_loss = test_model(x_test,y_test, global_model, comm_round)

len(cluster_model.layers[11].get_weights())

print(global_loss)

# print(global_model.get_weights().shape)
a=global_model.get_weights()
print(len(a))
print(len(a[0][0]))
# a=np.array(a,dtype=object)
print(a.shape[0])

print(a)

print(a[3])

for layer in global_model.layers:
    weights = np.array(layer.get_weights())
    print(weights.shape)



for layer in global_model.layers: print(layer.get_config(), layer.get_weights())

first_layer_weights = global_model.layers[0].get_weights()[0]
first_layer_biases  = global_model.layers[0].get_weights()[1]
# second_layer_weights = global_model.layers[1].get_weights()[0]
# second_layer_biases  = global_model.layers[1].get_weights()[1]

print(first_layer_weights.shape,first_layer_biases.shape)

print(len(global_model.layers))

for layer in global_model.layers:
    print(layer.input_shape,layer.output_shape)
    if(np.array(layer.get_weights()).shape[0]>=1):
      wt,bs=layer.get_weights()
      print(wt.shape,bs.shape)
      print(len(wt),len(wt[0]))

for layer in local_model.layers:
    print(layer.input_shape,layer.output_shape)
    if(np.array(layer.get_weights()).shape[0]>=1):
      wt,bs=layer.get_weights()
      print(wt.shape,bs.shape)
      print(len(wt),len(wt[0]))

print(local_model.get_weights())

abcd=local_model.get_weights()

for layer in local_model.layers:
    print(layer.input_shape,layer.output_shape)
    if((np.array(layer.get_weights()).shape[0])>=1):
      wt,bs=layer.get_weights()
      print(wt.shape,bs.shape)
      print(len(wt),len(wt[0]))



gbwt=np.array(global_model.get_weights())

print(gbwt[5].shape)

print(local_model.get_weights)

print(local_model.get_weights())

print(len(global_model.layers))

nmlayers=len(global_model.layers)
for i in range(1,nmlayers-1):
  if(len(global_model.layers[i].get_weights())>0):
    wt,bs=local_model.layers[i].get_weights()
    wt1,bs1=global_model.layers[i].get_weights()
    print(i)
    # print(wt.shape[2],bs.shape[0])
    # print(wt1.shape,bs1.shape)
    print(wt.shape)
    wt=wt1[:,:,:wt.shape[2],:wt.shape[3]]
    bs=bs1[:bs.shape[0]]
    local_model.layers[i].set_weights([wt,bs])

